
[project]
name = "llm_engineering"
version = "0.1.0"
description = "RAG pipelines using ZenML and Hugging Face"
authors = [{ name = "Alec Pippas", email = "your.email@domain.com" }]
dependencies = [
    "huggingface-hub>=0.30.2",
    "pymongo>=4.12.0",
    "qdrant-client>=1.13.3",
    "open-clip-torch>=2.32.0",
    "requests>=2.32.3",
    "sseclient>=0.0.27",
    "python-dotenv>=1.1.0",
    "decord>=0.6.0",
    "torchvision>=0.21.0",
    "loguru>=0.7.3",
    "pydantic-settings>=2.9.1",
    "zenml[server]==0.74.0",
    "datasets==3.0.1",
    "gradio==3.50.2",
    "fastapi==0.110.0",
    "sentence-transformers>=4.1.0",
    "chromedriver-autoinstaller>=0.6.4",
    "selenium>=4.31.0",
    "langchain-community>=0.3.21",
    "bs4>=0.0.2",
    "tiktoken>=0.9.0",
    "langchain-openai>=0.3.14",
    "accelerate>=0.26.0",
    "ipykernel>=6.29.5",
]


[tool.rye]
managed = true


[tool.setuptools.packages.find]
include = ["llm_engineering*"]

[tool.rye.scripts]
# Data pipelines
run-digital-data-etl-alex                 = "echo 'It is not supported anymore.'"
run-digital-data-etl-maxime               = "python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml"
run-digital-data-etl-paul                 = "python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml"
run-digital-data-etl                       = "python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml && python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml"
run-feature-engineering-pipeline          = "python -m tools.run --no-cache --run-feature-engineering"
run-generate-instruct-datasets-pipeline    = "python -m tools.run --no-cache --run-generate-instruct-datasets"
run-generate-preference-datasets-pipeline  = "python -m tools.run --no-cache --run-generate-preference-datasets"
run-end-to-end-data-pipeline              = "python -m tools.run --no-cache --run-end-to-end-data"

# Utility pipelines
run-export-artifact-to-json-pipeline      = "python -m tools.run --no-cache --run-export-artifact-to-json"
run-export-data-warehouse-to-json         = "python -m tools.data_warehouse --export-raw-data"
run-import-data-warehouse-from-json       = "python -m tools.data_warehouse --import-raw-data"

# Training pipelines
run-training-pipeline                     = "python -m tools.run --no-cache --run-training"
run-evaluation-pipeline                   = "python -m tools.run --no-cache --run-evaluation"

# Inference
call-rag-retrieval-module                 = "python -m tools.rag"
run-inference-ml-service                  = "uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload"
call-inference-ml-service                 = "bash -c '\"curl -X POST http://127.0.0.1:8000/rag -H 'Content-Type: application/json' -d '{\\\"query\\\":\\\"My name is Paul Iusztin. Could you draft a LinkedIn post discussing RAG systems? I am particularly interested in how RAG works and how it is integrated with vector DBs and LLMs.\\\"}'\"'"

# Local infrastructure
local-docker-infrastructure-up            = "docker compose up -d"
local-docker-infrastructure-down          = "docker compose stop"
local-zenml-server-down                   = "zenml logout --local"
local-zenml-server-up                     = "zenml login --local"
local-infrastructure-up                   = "docker compose up -d && zenml logout --local && zenml login --local"
local-infrastructure-down                 = "docker compose stop && zenml logout --local"

# ZenML stacks
set-local-stack                           = "zenml stack set default"
set-aws-stack                             = "zenml stack set aws-stack"
set-asynchronous-runs                     = "zenml orchestrator update aws-stack --synchronous=False"
zenml-server-disconnect                   = "zenml disconnect"

# Settings
export-settings-to-zenml                  = "python -m tools.run --export-settings"
delete-settings-zenml                     = "zenml secret delete settings"

# SageMaker tasks
create-sagemaker-role                     = "python -m llm_engineering.infrastructure.aws.roles.create_sagemaker_role"
create-sagemaker-execution-role           = "python -m llm_engineering.infrastructure.aws.roles.create_execution_role"
deploy-inference-endpoint                 = "python -m llm_engineering.infrastructure.aws.deploy.huggingface.run"
test-sagemaker-endpoint                   = "python -m llm_engineering.model.inference.test"
delete-sagemaker-endpoint                 = "python -m llm_engineering.infrastructure.aws.deploy.delete_sagemaker_endpoint"

# Docker helpers
build-docker-image                        = "docker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile ."
run-docker-end-to-end-data-pipeline       = "docker run --rm --network host --shm-size=2g --env-file .env llmtwin"
bash-docker-container                     = "docker run --rm -it --network host --env-file .env llmtwin bash"

# QA
lint-check                                = "ruff check ."
format-check                              = "ruff format --check ."
lint-check-docker                         = "sh -c 'docker run --rm -i hadolint/hadolint < Dockerfile'"
gitleaks-check                            = "docker run -v .:/src zricethezav/gitleaks:latest dir /src/llm_engineering"
lint-fix                                  = "ruff check --fix ."
format-fix                                = "ruff format ."

test                                      = "pytest tests/"
