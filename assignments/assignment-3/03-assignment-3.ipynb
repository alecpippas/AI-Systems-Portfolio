{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Object Detection\n",
    "By: Alec Pippas (awp251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Object Detection of Car in Short Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_to_frames(video_path, frames_folder):\n",
    "    \"\"\"\n",
    "    Splits a video into frames and saves them into the specified folder.\n",
    "    \"\"\"\n",
    "    os.makedirs(frames_folder, exist_ok=True)\n",
    "    captured_vid = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        #reads one frame per iteration, .read() returns a tuple (ret, frame)\n",
    "        # ret: boolean indicating if frame was successfully read\n",
    "        # frame: frame image stored as NumPy array\n",
    "        ret, frame = captured_vid.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(frames_folder, f'frame_{frame_count:06d}.jpg'), frame) # save current frame withi the frames_folder\n",
    "        frame_count += 1\n",
    "\n",
    "    #free up resources (file handles, memory) associated with the cv2.VideoCaptuer object\n",
    "    captured_vid.release() \n",
    "    return frame_count\n",
    "\n",
    "\n",
    "def detect_and_annotate(frames_folder, processed_folder, model, total_frames):\n",
    "    \"\"\"\n",
    "    Performs object detection on each frame using a YOLO model,\n",
    "    draws bounding boxes (for cars) with centroids, and saves the new frames.\n",
    "    \"\"\"\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        frame_path = os.path.join(frames_folder, f'frame_{i:06d}.jpg')\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "        \n",
    "        # Run object detection\n",
    "        results = model(frame)\n",
    "\n",
    "        # Draw bounding boxes and centroids\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                class_idx = int(box.cls[0])       # Integer index of the predicted class\n",
    "                label = r.names[class_idx]        # Class label (e.g., \"car\", \"person\", etc.)\n",
    "                if label == \"car\":               # Filter for the \"car\" class\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coords\n",
    "                    conf = box.conf[0].item()               # Confidence score\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    # Class label + confidence\n",
    "                    text = f\"{label} {conf:.2f}\"\n",
    "                    cv2.putText(frame, text, (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                    # Draw centroid\n",
    "                    cx = (x1 + x2) // 2\n",
    "                    cy = (y1 + y2) // 2\n",
    "                    cv2.circle(frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "        \n",
    "        # Save the annotated frame\n",
    "        cv2.imwrite(os.path.join(processed_folder, f'frame_{i:06d}.jpg'), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_video_from_frames(processed_folder, output_video, total_frames, fps=30):\n",
    "    \"\"\"\n",
    "    Rebuilds a video from processed frames.\n",
    "    \"\"\"\n",
    "    # Read the first frame to get size info\n",
    "    first_frame_path = os.path.join(processed_folder, 'frame_000000.jpg')\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    \n",
    "    #raise execption if the frame was not read (may indicate the frame was not extracted/processed correctly)\n",
    "    if first_frame is None:\n",
    "        raise FileNotFoundError(f\"Could not read the file: {first_frame_path}\")\n",
    "\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # Create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID', etc.\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write each processed frame to the new video\n",
    "    for i in range(total_frames):\n",
    "        processed_frame_path = os.path.join(processed_folder, f'frame_{i:06d}.jpg')\n",
    "        processed_frame = cv2.imread(processed_frame_path)\n",
    "        if processed_frame is not None:\n",
    "            out.write(processed_frame)\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 train, 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 20.0ms\n",
      "Speed: 2.3ms preprocess, 20.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 19.5ms\n",
      "Speed: 1.7ms preprocess, 19.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 19.5ms\n",
      "Speed: 1.9ms preprocess, 19.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 19.4ms\n",
      "Speed: 1.5ms preprocess, 19.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 19.3ms\n",
      "Speed: 1.5ms preprocess, 19.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 17.8ms\n",
      "Speed: 1.6ms preprocess, 17.8ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.1ms\n",
      "Speed: 1.9ms preprocess, 14.1ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 13.6ms\n",
      "Speed: 2.0ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 13.5ms\n",
      "Speed: 1.8ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 13.6ms\n",
      "Speed: 1.9ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 truck, 14.1ms\n",
      "Speed: 2.2ms preprocess, 14.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 truck, 13.9ms\n",
      "Speed: 1.7ms preprocess, 13.9ms inference, 13.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.1ms\n",
      "Speed: 1.7ms preprocess, 14.1ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.0ms\n",
      "Speed: 1.9ms preprocess, 10.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.3ms\n",
      "Speed: 1.8ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.8ms\n",
      "Speed: 1.7ms preprocess, 10.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 truck, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.9ms\n",
      "Speed: 1.1ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.0ms\n",
      "Speed: 1.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 14.2ms\n",
      "Speed: 1.2ms preprocess, 14.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 train, 8.5ms\n",
      "Speed: 1.7ms preprocess, 8.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.3ms\n",
      "Speed: 1.1ms preprocess, 8.3ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.4ms\n",
      "Speed: 1.6ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.1ms\n",
      "Speed: 1.2ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.1ms\n",
      "Speed: 1.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 10.9ms\n",
      "Speed: 1.3ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.6ms\n",
      "Speed: 1.3ms preprocess, 9.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 7.8ms\n",
      "Speed: 1.2ms preprocess, 7.8ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.0ms\n",
      "Speed: 1.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 7.8ms\n",
      "Speed: 1.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 truck, 8.8ms\n",
      "Speed: 1.1ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.1ms\n",
      "Speed: 1.2ms preprocess, 9.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 7.7ms\n",
      "Speed: 1.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 1 truck, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.3ms\n",
      "Speed: 1.2ms preprocess, 9.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 9.0ms\n",
      "Speed: 1.3ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 7.4ms\n",
      "Speed: 1.1ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 bus, 14.8ms\n",
      "Speed: 1.2ms preprocess, 14.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.6ms\n",
      "Speed: 1.0ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 11.3ms\n",
      "Speed: 1.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 14.0ms\n",
      "Speed: 1.3ms preprocess, 14.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 8.5ms\n",
      "Speed: 1.2ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.9ms\n",
      "Speed: 1.1ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 12.4ms\n",
      "Speed: 1.2ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 14.3ms\n",
      "Speed: 1.7ms preprocess, 14.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.2ms\n",
      "Speed: 1.2ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 12.7ms\n",
      "Speed: 1.7ms preprocess, 12.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.1ms\n",
      "Speed: 1.3ms preprocess, 7.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.0ms\n",
      "Speed: 1.5ms preprocess, 8.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 8.3ms\n",
      "Speed: 1.2ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.9ms\n",
      "Speed: 1.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.1ms\n",
      "Speed: 1.2ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.3ms\n",
      "Speed: 1.3ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 16.8ms\n",
      "Speed: 1.3ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.3ms\n",
      "Speed: 1.2ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 12.4ms\n",
      "Speed: 1.1ms preprocess, 12.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 13.6ms\n",
      "Speed: 1.2ms preprocess, 13.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.5ms\n",
      "Speed: 1.2ms preprocess, 9.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.4ms\n",
      "Speed: 1.2ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.7ms\n",
      "Speed: 1.1ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.1ms\n",
      "Speed: 1.2ms preprocess, 8.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 6.8ms\n",
      "Speed: 1.2ms preprocess, 6.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 13.7ms\n",
      "Speed: 1.2ms preprocess, 13.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 6.9ms\n",
      "Speed: 1.2ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 7.7ms\n",
      "Speed: 1.3ms preprocess, 7.7ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 9.0ms\n",
      "Speed: 1.1ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 7.7ms\n",
      "Speed: 1.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.9ms\n",
      "Speed: 1.5ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 12.5ms\n",
      "Speed: 1.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 9.1ms\n",
      "Speed: 1.3ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.4ms\n",
      "Speed: 1.1ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 7.9ms\n",
      "Speed: 1.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 6.9ms\n",
      "Speed: 1.1ms preprocess, 6.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 7.5ms\n",
      "Speed: 1.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 7.6ms\n",
      "Speed: 1.3ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 9.0ms\n",
      "Speed: 1.2ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 6.8ms\n",
      "Speed: 1.2ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.7ms\n",
      "Speed: 1.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 8.1ms\n",
      "Speed: 1.1ms preprocess, 8.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 7.4ms\n",
      "Speed: 1.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 1 truck, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 7.8ms\n",
      "Speed: 1.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.8ms\n",
      "Speed: 1.0ms preprocess, 6.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.0ms\n",
      "Speed: 1.1ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.6ms\n",
      "Speed: 1.4ms preprocess, 6.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 12.1ms\n",
      "Speed: 1.2ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 9.8ms\n",
      "Speed: 1.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.3ms\n",
      "Speed: 1.1ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 7.4ms\n",
      "Speed: 1.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.1ms\n",
      "Speed: 1.3ms preprocess, 11.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 6.4ms\n",
      "Speed: 1.1ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.1ms\n",
      "Speed: 1.2ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 truck, 12.4ms\n",
      "Speed: 1.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 6.7ms\n",
      "Speed: 1.1ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.3ms\n",
      "Speed: 1.1ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.8ms\n",
      "Speed: 1.4ms preprocess, 7.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.3ms\n",
      "Speed: 1.3ms preprocess, 9.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 car, 6.9ms\n",
      "Speed: 1.2ms preprocess, 6.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#3. Detect and annotate frames with bounding box + centroid for \"car\" objects\u001b[39;00m\n\u001b[1;32m     10\u001b[0m processed_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdetect_and_annotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#4. Rebuild the annotated frames into a new video\u001b[39;00m\n\u001b[1;32m     14\u001b[0m output_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mdetect_and_annotate\u001b[0;34m(frames_folder, processed_folder, model, total_frames)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run object detection\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and centroids\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:214\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:319\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 319\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/dev_projects/AI_Course_Projects/CS-GY6613-Assignment-awp251_clone/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:155\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    153\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform(im))\n\u001b[1;32m    154\u001b[0m     im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(im)\n\u001b[1;32m    158\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1) Split the video into frames\n",
    "video_path = \"Assignment_3_video_ActiveTrack\"\n",
    "frames_folder = \"extracted_frames\"\n",
    "total_frames = split_video_to_frames(video_path, frames_folder)\n",
    "\n",
    "#2) Load a YOLO model (YOLOv8n pretrained on COCO)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "#3. Detect and annotate frames with bounding box + centroid for \"car\" objects\n",
    "processed_folder = \"processed_frames\"\n",
    "detect_and_annotate(frames_folder, processed_folder, model, total_frames)\n",
    "                    \n",
    "#4. Rebuild the annotated frames into a new video\n",
    "output_video = \"output_video.mp4\"\n",
    "rebuild_video_from_frames(processed_folder, output_video, total_frames, fps=30)\n",
    "print(f\"Object Detection with bounding boxes now complete. Annotated video has been savet to: {output_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rye - assignment_3)",
   "language": "python",
   "name": "assignment_3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
